{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "project_part2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Galzi1/DataDiscoveryNNProj/blob/master/project_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDzKT-jJCqlb",
        "colab_type": "text"
      },
      "source": [
        "# Knowledge Data Discovery and Neural Networks : Final Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RNH1oP_Cqlc",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we will prepare data and run several algorithms for classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT8nIf--Cqlc",
        "colab_type": "text"
      },
      "source": [
        "# 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POfS3zjTCqld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add more packages in this section\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dredJsfHCqlg",
        "colab_type": "text"
      },
      "source": [
        "# 2. data preperation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JKEhltnCqlg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data = pd.read_csv(\"data/adult.data\", header = None)\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/Galzi1/DataDiscoveryNNProj/master/data/adult.data\", header = None)\n",
        "cols = ['age', 'workclass', 'fnlwgt','education','education_num','marital_status','occupation','relationship','race','sex','capital_gain', 'capital_loss'\n",
        "    ,'hours_per_week','native_country','y']\n",
        "data.columns = cols"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARDQJAR7Cqlj",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Categorical feature handling\n",
        "* Name two machine learning algorithms that can deal with categorical features without special handling?\n",
        "\n",
        "special handling = one hot encoding exc.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suReLsHsCqlj",
        "colab_type": "text"
      },
      "source": [
        "#### YOUR VERBAL SOLUTION HERE\n",
        "* Naive Bayes (explain some)\n",
        "* Some Tree-based algorithm (specify one and explain)\n",
        "\n",
        "\n",
        "#### END YOUR VERBAL SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0uySf-WCqlk",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 One hot encoding \n",
        "\n",
        "* Remove the y column from the data variable and save it to the variable y\n",
        "* Transform the categorical columns to one hot encoding\n",
        "\n",
        "You may find get_dummies function in pandas useful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMmmDqo-Cqlk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "8f0db87e-b873-4d0d-d22a-fffb59a77043"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "y = data['y']\n",
        "X = data.drop(['y'], axis=1)\n",
        "\n",
        "obj_cols = list(X.select_dtypes(include=['object']).columns)\n",
        "\n",
        "X = pd.get_dummies(X, columns=obj_cols, drop_first=True)\n",
        "\n",
        "X.head()\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education_num</th>\n",
              "      <th>capital_gain</th>\n",
              "      <th>capital_loss</th>\n",
              "      <th>hours_per_week</th>\n",
              "      <th>workclass_ Federal-gov</th>\n",
              "      <th>workclass_ Local-gov</th>\n",
              "      <th>workclass_ Never-worked</th>\n",
              "      <th>workclass_ Private</th>\n",
              "      <th>workclass_ Self-emp-inc</th>\n",
              "      <th>workclass_ Self-emp-not-inc</th>\n",
              "      <th>workclass_ State-gov</th>\n",
              "      <th>workclass_ Without-pay</th>\n",
              "      <th>education_ 11th</th>\n",
              "      <th>education_ 12th</th>\n",
              "      <th>education_ 1st-4th</th>\n",
              "      <th>education_ 5th-6th</th>\n",
              "      <th>education_ 7th-8th</th>\n",
              "      <th>education_ 9th</th>\n",
              "      <th>education_ Assoc-acdm</th>\n",
              "      <th>education_ Assoc-voc</th>\n",
              "      <th>education_ Bachelors</th>\n",
              "      <th>education_ Doctorate</th>\n",
              "      <th>education_ HS-grad</th>\n",
              "      <th>education_ Masters</th>\n",
              "      <th>education_ Preschool</th>\n",
              "      <th>education_ Prof-school</th>\n",
              "      <th>education_ Some-college</th>\n",
              "      <th>marital_status_ Married-AF-spouse</th>\n",
              "      <th>marital_status_ Married-civ-spouse</th>\n",
              "      <th>marital_status_ Married-spouse-absent</th>\n",
              "      <th>marital_status_ Never-married</th>\n",
              "      <th>marital_status_ Separated</th>\n",
              "      <th>marital_status_ Widowed</th>\n",
              "      <th>occupation_ Adm-clerical</th>\n",
              "      <th>occupation_ Armed-Forces</th>\n",
              "      <th>occupation_ Craft-repair</th>\n",
              "      <th>occupation_ Exec-managerial</th>\n",
              "      <th>occupation_ Farming-fishing</th>\n",
              "      <th>...</th>\n",
              "      <th>native_country_ Canada</th>\n",
              "      <th>native_country_ China</th>\n",
              "      <th>native_country_ Columbia</th>\n",
              "      <th>native_country_ Cuba</th>\n",
              "      <th>native_country_ Dominican-Republic</th>\n",
              "      <th>native_country_ Ecuador</th>\n",
              "      <th>native_country_ El-Salvador</th>\n",
              "      <th>native_country_ England</th>\n",
              "      <th>native_country_ France</th>\n",
              "      <th>native_country_ Germany</th>\n",
              "      <th>native_country_ Greece</th>\n",
              "      <th>native_country_ Guatemala</th>\n",
              "      <th>native_country_ Haiti</th>\n",
              "      <th>native_country_ Holand-Netherlands</th>\n",
              "      <th>native_country_ Honduras</th>\n",
              "      <th>native_country_ Hong</th>\n",
              "      <th>native_country_ Hungary</th>\n",
              "      <th>native_country_ India</th>\n",
              "      <th>native_country_ Iran</th>\n",
              "      <th>native_country_ Ireland</th>\n",
              "      <th>native_country_ Italy</th>\n",
              "      <th>native_country_ Jamaica</th>\n",
              "      <th>native_country_ Japan</th>\n",
              "      <th>native_country_ Laos</th>\n",
              "      <th>native_country_ Mexico</th>\n",
              "      <th>native_country_ Nicaragua</th>\n",
              "      <th>native_country_ Outlying-US(Guam-USVI-etc)</th>\n",
              "      <th>native_country_ Peru</th>\n",
              "      <th>native_country_ Philippines</th>\n",
              "      <th>native_country_ Poland</th>\n",
              "      <th>native_country_ Portugal</th>\n",
              "      <th>native_country_ Puerto-Rico</th>\n",
              "      <th>native_country_ Scotland</th>\n",
              "      <th>native_country_ South</th>\n",
              "      <th>native_country_ Taiwan</th>\n",
              "      <th>native_country_ Thailand</th>\n",
              "      <th>native_country_ Trinadad&amp;Tobago</th>\n",
              "      <th>native_country_ United-States</th>\n",
              "      <th>native_country_ Vietnam</th>\n",
              "      <th>native_country_ Yugoslavia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>77516</td>\n",
              "      <td>13</td>\n",
              "      <td>2174</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>83311</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>215646</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>234721</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>338409</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  fnlwgt  ...  native_country_ Vietnam  native_country_ Yugoslavia\n",
              "0   39   77516  ...                        0                           0\n",
              "1   50   83311  ...                        0                           0\n",
              "2   38  215646  ...                        0                           0\n",
              "3   53  234721  ...                        0                           0\n",
              "4   28  338409  ...                        0                           0\n",
              "\n",
              "[5 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bh315H_Cqlm",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 Train test split\n",
        "\n",
        "* Convert the y column - replace <=50K with 0 and >50K with 1 \n",
        "* Split the dataset into train and test set (use 15% for the test set)\n",
        "\n",
        "at the end, make sure you have the following variables:\n",
        "\n",
        "* X\n",
        "* y\n",
        "* X_train\n",
        "* X_test\n",
        "* y_train\n",
        "* y_test\n",
        "\n",
        "You may find sklearn train_test_split useful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8x0S20LCqln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "Y = labelencoder.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRA1aiKyCqlo",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 Numeric feature normalization\n",
        "\n",
        "* Scale the numeric features to to have zero mean (z score normalization)\n",
        "\n",
        "- Don't scale the boolean features\n",
        "\n",
        "You may find sklearn StandardScaler useful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC62JGeECqlp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "aed8508c-1bf3-4317-d705-a9e4a7d8a071"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "def standardize_column(col):\n",
        "    name = col.name\n",
        "    x = col.values\n",
        "    x = x.reshape(-1, 1)\n",
        "    scaler = preprocessing.StandardScaler()\n",
        "    x_scaled = scaler.fit_transform(x)\n",
        "    print(np.count_nonzero(np.isnan(x_scaled)))\n",
        "    srs_standardized = pd.Series(x_scaled.flatten())\n",
        "    srs_standardized.name = name\n",
        "    return srs_standardized\n",
        "\n",
        "\n",
        "for c in X_train.columns:\n",
        "  if X_train[c].dtype == np.int64:\n",
        "    X_train[c] = standardize_column(X_train[c])\n",
        "    \n",
        "### END YOUR CODE"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoJomS7QCqlr",
        "colab_type": "text"
      },
      "source": [
        "# 3 Models\n",
        "\n",
        "Create a function `cv(x, y, model)` (cv stands for cross validation) that gets a model (sklearn classifier) and the data.  \n",
        "The function should fit the model using k fold cross validation with k = 5, and print the 'roc_auc' on each fold (which is the scoring parameter for the cross val_score function).\n",
        "\n",
        "\n",
        "[Computing cross-validated metrics](https://scikit-learn.org/stable/modules/cross_validation.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9EY53jyCqls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## YOUR CODE HERE\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import metrics\n",
        "\n",
        "def cv(x, y, model):\n",
        "  print(cross_val_score(model, X, y, cv=5, scoring='roc_auc'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## END YOUR CODE"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPQdcsp2Cqlu",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Baseline model - logistic regression\n",
        "Our first algorithm will be logistic regression, since it's always nice to know how well can we do with a simple algorithm.\n",
        "* Should we use class_weight = 'balanced' in sklearn logistic regression? why?\n",
        "* Check how the results differ with and without the 'balanced' parameter - use `cv(x, y, model)` for that purpose, explain your answer\n",
        "* Fit a model on X_train, y_train and plot the precision recall curve on the test data.\n",
        "* If we would plot this curve on each fold (of the 5 folds in the cross validation), would we get exactly the same plot?\n",
        "\n",
        "Do not change other hyperparameters\n",
        "\n",
        "[precision recall curve](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWcmrdMbCqlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## END YOUR CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx0IU4EaCqlw",
        "colab_type": "text"
      },
      "source": [
        "#### YOUR VERBAL SOLUTION HERE\n",
        "\n",
        "\n",
        "\n",
        "#### END YOUR VERBAL SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tFF-GsLCqlx",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 ANN\n",
        "ANN (MLPClassifier in sklearn):\n",
        "\n",
        "* Fit a model on X_train, y_train and print the AUC and the Log Loss on the train and test data.\n",
        "* Explain your results - are they better or worse than the baseline? try to explain why\n",
        "\n",
        "Do not change hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kHOtZsqCqlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8WBLaBUCqlz",
        "colab_type": "text"
      },
      "source": [
        "#### YOUR VERBAL SOLUTION HERE\n",
        "\n",
        "\n",
        "\n",
        "#### END YOUR VERBAL SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHYMEb55Cqlz",
        "colab_type": "text"
      },
      "source": [
        "## 3.3 Random forest\n",
        "Random Forest classifier:\n",
        "\n",
        "* Fit a model on X_train, y_train and print the AUC and the Log Loss on the train and test data.\n",
        "* Explain your results - are they better or worse than the baseline? try to explain why\n",
        "\n",
        "Do not change hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPBPod-yCql0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "31f9ac97-9bf8-4d19-e3a1-9d3f94aac776"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
        "print(metrics.auc(fpr, tpr))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-cd529725c8a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \"\"\"\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2pTnSKtCql2",
        "colab_type": "text"
      },
      "source": [
        "#### YOUR VERBAL SOLUTION HERE\n",
        "\n",
        "\n",
        "\n",
        "#### END YOUR VERBAL SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwOwBPGHCql2",
        "colab_type": "text"
      },
      "source": [
        "## 3.4 Confusion matrix\n",
        "Plot/print the confusion matrix of the random forest model on the test data\n",
        "\n",
        "Explain shortly your results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c_ImO_nCql3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "d3e8785c-5064-4f30-e0f9-3fc41220dd89"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "class_names = y.unique()\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
        "                  (\"Normalized confusion matrix\", 'true')]\n",
        "for title, normalize in titles_options:\n",
        "    disp = plot_confusion_matrix(clf, X_test, y_test,\n",
        "                                 display_labels=class_names,\n",
        "                                 cmap=plt.cm.Blues,\n",
        "                                 normalize=normalize)\n",
        "    disp.ax_.set_title(title)\n",
        "\n",
        "    print(title)\n",
        "    print(disp.confusion_matrix)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-e6f84602cef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                                  \u001b[0mdisplay_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                  \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                  normalize=normalize)\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mdisp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0max_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_plot/confusion_matrix.py\u001b[0m in \u001b[0;36mplot_confusion_matrix\u001b[0;34m(estimator, X, y_true, labels, sample_weight, normalize, display_labels, include_values, xticks_rotation, values_format, cmap, ax)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"plot_confusion_matrix only supports classifiers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     cm = confusion_matrix(y_true, y_pred, sample_weight=sample_weight,\n\u001b[1;32m    187\u001b[0m                           labels=labels, normalize=normalize)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \"\"\"\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0mcorresponds\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \"\"\"\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_ihz7YhCql4",
        "colab_type": "text"
      },
      "source": [
        "#### YOUR VERBAL SOLUTION HERE\n",
        "\n",
        "\n",
        "\n",
        "#### END YOUR VERBAL SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-54VcMmCql5",
        "colab_type": "text"
      },
      "source": [
        "## 3.4 Global feature importance of the random forest and SHAP\n",
        "\n",
        "* Plot the global feature importance of the features - use SHAP for this purpsose\n",
        "* Choose two samples from the dataset and plot/print the local explanations for this samples. Explain which features\n",
        "are important\n",
        "* Explain in few words how this feature importance is calculated\n",
        "\n",
        "**SHAP can be very slow on the random forest model. Thus, train another model with shallow trees (depth <7 for example) and you can also compute the SHAP values on a small data set. If it is yet very slow you, try to fit a different classifier for this purpose (MLP for example or GradientBoostingClassifier). You can also discard the binary features (which derived from the categorical features) for this purpose and stay only with the numerical ones.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_DmqsyoCql5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nThOjCCCql7",
        "colab_type": "text"
      },
      "source": [
        "#### YOUR VERBAL SOLUTION HERE\n",
        "\n",
        "\n",
        "\n",
        "#### END YOUR VERBAL SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWSZ3aJUCql7",
        "colab_type": "text"
      },
      "source": [
        "## 4 Clustering\n",
        "Imagine that we don't really know the true labels -> we need to use unsupervised machine learning.\n",
        "\n",
        "* Perform k means on X_train with k = 2. Is our clusters represent rich and poor people (does one cluster represent 'rich' ('>= 50k') people and the other 'poor' people)?\n",
        "\n",
        "* Do the same with dbscan, understand how many clusters did you get and the proportion of 'rich' and 'poor' people in each cluster.\n",
        "\n",
        "* Plot the clusters using pca (only for the kmeans). Are they seperated in the PCA dimension?\n",
        " \n",
        "This question is more open minded and you can (recomended) use graphs that explain how well did the clustering work. Did it work well?? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Itm1Wrkr-26S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "78b3a894-0258-4c5b-ce16-0d7223a9ba66"
      },
      "source": [
        "print(list(X_train.loc[pd.isna(X_train[\"age\"]), :].index))\n",
        "print(X_train.loc[28956, :])\n",
        "# print(X_train.isna().sum())\n",
        "# print(\"########################\")\n",
        "# # 2)\n",
        "# for c in X_train.columns:\n",
        "#   if not np.issubdtype(X_train[c].dtype, np.number):\n",
        "#     print(f'Column = {c}; unique values = {sorted(X_train[c].unique())}')\n",
        "# print(\"########################\")\n",
        "# # 3)\n",
        "# for c in X_train.columns:\n",
        "#   if not np.issubdtype(X_train[c].dtype, np.number):\n",
        "#   print(f'Column = {c}; Number of \"?\" = {X_train[X_train[c] == \" ?\"].shape[0]}')\n",
        "# print(\"########################\")"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[26440, 28868, 31846, 31234, 31513, 31936, 27407, 30245, 29818, 31114, 32134, 32148, 32030, 26816, 28882, 30671, 30280, 31269, 27136, 28965, 26141, 29334, 31755, 30298, 29711, 30894, 30253, 26746, 29853, 29798, 27561, 27040, 28837, 26397, 30563, 27887, 28487, 26505, 28654, 31093, 27346, 28205, 31862, 32082, 29178, 26888, 26807, 29493, 30217, 26280, 29855, 30045, 28933, 31409, 27100, 28850, 27000, 29784, 28427, 30254, 27580, 26147, 28925, 28662, 30445, 26229, 30655, 26633, 26724, 27697, 27693, 26928, 28738, 28014, 31395, 32383, 27316, 26441, 28198, 26649, 27790, 32033, 31502, 32185, 31554, 30524, 27140, 31251, 29717, 26572, 31029, 32129, 31271, 32417, 28892, 27847, 31007, 32127, 29304, 30618, 31990, 28444, 27322, 29246, 32456, 31984, 30647, 30814, 28239, 31890, 32258, 26573, 29854, 27624, 29546, 29758, 30458, 30898, 30408, 27907, 27838, 31991, 30240, 30272, 28089, 27812, 31466, 29864, 27614, 31480, 27026, 30159, 31659, 27936, 27423, 31321, 28298, 29057, 26910, 31518, 31254, 30460, 29877, 28004, 29486, 31913, 26362, 27355, 31250, 27811, 32240, 32047, 28276, 26942, 27437, 28752, 30342, 32200, 31764, 29435, 29228, 27099, 26630, 28680, 30451, 26655, 32186, 31048, 26320, 27049, 28782, 29097, 29702, 30106, 26495, 27269, 30881, 26169, 27846, 27554, 32354, 30672, 32465, 31895, 31483, 27551, 26800, 27281, 28152, 27843, 30602, 26713, 29925, 30545, 31189, 27638, 29272, 32313, 27182, 26432, 32532, 30347, 32165, 26157, 30394, 26257, 28709, 28768, 26631, 32524, 30380, 31729, 26575, 28159, 29624, 26389, 32193, 29497, 30613, 26074, 29087, 26129, 29746, 32380, 26241, 26600, 27477, 30977, 26652, 26698, 28110, 27010, 31804, 32298, 27687, 26834, 26325, 30000, 29091, 28438, 32043, 28945, 30456, 31940, 30821, 28819, 28702, 30392, 29141, 27315, 29378, 31419, 26791, 27685, 31465, 30901, 32254, 29020, 31530, 27366, 26948, 31416, 30778, 27227, 27441, 31193, 28187, 31965, 26623, 30774, 29578, 31011, 29386, 32375, 27112, 29346, 27324, 29362, 30664, 27054, 31780, 27001, 32364, 26395, 28563, 27931, 31379, 32332, 31712, 29860, 28011, 26691, 31978, 31183, 31303, 28888, 26134, 29640, 30953, 26806, 30480, 30330, 30534, 27821, 26107, 32448, 30278, 28406, 30053, 26661, 28862, 28165, 26962, 30646, 29498, 32104, 27724, 27921, 30586, 32545, 29725, 29306, 32450, 28469, 30350, 29232, 29712, 26925, 30276, 29763, 28935, 32497, 27660, 29419, 29350, 31962, 28175, 29276, 27759, 26798, 32521, 31508, 30931, 27985, 26159, 27329, 30998, 31983, 28802, 31596, 27345, 31231, 28743, 32405, 27641, 30689, 31957, 27385, 30929, 26216, 32463, 28579, 27296, 26624, 29215, 28413, 26967, 30204, 29433, 32514, 29121, 26236, 29289, 31146, 29912, 27426, 29104, 29739, 29809, 31617, 28473, 26726, 27879, 28139, 30905, 31040, 30135, 29944, 26437, 26881, 31331, 27945, 30249, 28157, 30128, 26991, 28467, 26061, 27914, 26100, 32395, 27674, 28490, 27581, 32074, 30095, 29454, 30281, 31089, 28664, 32091, 30222, 30418, 30183, 31308, 31361, 28539, 28913, 31963, 30097, 28542, 27167, 27577, 31870, 28878, 30767, 31867, 26908, 31598, 31806, 28366, 29128, 26422, 30502, 28143, 29762, 29904, 27623, 32138, 27029, 27797, 32171, 30014, 32145, 27098, 29184, 30152, 29409, 31302, 28404, 28779, 31754, 32266, 28066, 27939, 30703, 31753, 30012, 29181, 27289, 26052, 31585, 26443, 27450, 29948, 30307, 26959, 27439, 29427, 28246, 29213, 26459, 29483, 32239, 31564, 27539, 32157, 28308, 32552, 28955, 32146, 26680, 29360, 30902, 30512, 31431, 27079, 31903, 29795, 29376, 28241, 29113, 28105, 27435, 27190, 28330, 30367, 26644, 28128, 28017, 27526, 32111, 30093, 28060, 32120, 31902, 31180, 32093, 27114, 29050, 28902, 28211, 32536, 27402, 28169, 27604, 30766, 31603, 32464, 26863, 29314, 27747, 27217, 30818, 28006, 26993, 31323, 31661, 31422, 26059, 30938, 26855, 30992, 28765, 28253, 29727, 27671, 29254, 29122, 31415, 28983, 28208, 30552, 28500, 32106, 30078, 30830, 32458, 27353, 28368, 30667, 28048, 27458, 26699, 27394, 28178, 28506, 27563, 28920, 32527, 27250, 31551, 31536, 26552, 29756, 30209, 29953, 30614, 26247, 26149, 29509, 28338, 28980, 28156, 30546, 30474, 29561, 27176, 30383, 27736, 30662, 28601, 27975, 27358, 27538, 27173, 31098, 27996, 30519, 30195, 32051, 26381, 30140, 30046, 31522, 27591, 27784, 26803, 27925, 30236, 30348, 29381, 26579, 28658, 28796, 27038, 30819, 32103, 31353, 29192, 29328, 31261, 27028, 28912, 28581, 31847, 26125, 26841, 27706, 26766, 29471, 31877, 27291, 26494, 26277, 29145, 27172, 27705, 32088, 32156, 32297, 29826, 29089, 26702, 30385, 27158, 27487, 28887, 32454, 31161, 27122, 28901, 30568, 28503, 29699, 26206, 30550, 32382, 27025, 31588, 27106, 28104, 29549, 30968, 27030, 26611, 29671, 28477, 31559, 28220, 32302, 26871, 29452, 29606, 27661, 26570, 26285, 32477, 30865, 27356, 28465, 27053, 31050, 29576, 31563, 30736, 29651, 29219, 29885, 29747, 31472, 27063, 31336, 32257, 30202, 32324, 28130, 29931, 29833, 26813, 31860, 29782, 30995, 32406, 31132, 32343, 26173, 27459, 28478, 30428, 29604, 28109, 31099, 31481, 32110, 29972, 29868, 31115, 31038, 26167, 26053, 28000, 29006, 31732, 30365, 31728, 29709, 31886, 31624, 29100, 27986, 29241, 28233, 27791, 30605, 29794, 28146, 29587, 26675, 30331, 29082, 29609, 27445, 27361, 27989, 30196, 30218, 29403, 29005, 31242, 30154, 29485, 32037, 26194, 30573, 29124, 31133, 29930, 31934, 26251, 28151, 30612, 32245, 28468, 27417, 30198, 31430, 30906, 30967, 30492, 30771, 30119, 29477, 27895, 28318, 31652, 32471, 31621, 28106, 32314, 28386, 32144, 32205, 28769, 26070, 29852, 32187, 32169, 26523, 30038, 27515, 27802, 30660, 26890, 26759, 26793, 31235, 28763, 31341, 27633, 28494, 28095, 28948, 26547, 31152, 28193, 32132, 28742, 32424, 31707, 28789, 28773, 26732, 28523, 32237, 29147, 29032, 29395, 26776, 26672, 26977, 28886, 27642, 29267, 30318, 31398, 32123, 26390, 32553, 27142, 27347, 30270, 31570, 29062, 26695, 29060, 29501, 27816, 31835, 29370, 31073, 29893, 29505, 30633, 32139, 26262, 31223, 29730, 26606, 27830, 29336, 27933, 27619, 30167, 29836, 28876, 29840, 30780, 30158, 29291, 31036, 27328, 27216, 29490, 26692, 27703, 32557, 28771, 27916, 31275, 30258, 32503, 27447, 26427, 30485, 30511, 28998, 30405, 29705, 28002, 29994, 28745, 31869, 31495, 29194, 31704, 28818, 30312, 31276, 27874, 32248, 31829, 26558, 29333, 31858, 26711, 29075, 32425, 30961, 28823, 29491, 27067, 31074, 28610, 29590, 26335, 31774, 30397, 27362, 26379, 31750, 32126, 31010, 27951, 28785, 26203, 26645, 29675, 26882, 28385, 31479, 29884, 31771, 32250, 28797, 29127, 26992, 31746, 26366, 29156, 26877, 29901, 28516, 26364, 31639, 30595, 26150, 27616, 31065, 26536, 28087, 27348, 31627, 27266, 30504, 31317, 27689, 27102, 30029, 26339, 28953, 30987, 31672, 30653, 28993, 28728, 28909, 27251, 29256, 28900, 27560, 26077, 28616, 31387, 31197, 29038, 26964, 29407, 29729, 29363, 27230, 31795, 31710, 32505, 26883, 31037, 31458, 29418, 28172, 26671, 30215, 32370, 26499, 29394, 31058, 30727, 27597, 27719, 28640, 29063, 31112, 27022, 32434, 31181, 28893, 31464, 29067, 31613, 31545, 26670, 29862, 29837, 31313, 26174, 27901, 30483, 28524, 27920, 29384, 29965, 30438, 26830, 27909, 30781, 32303, 30654, 27675, 31599, 30208, 29915, 29420, 29266, 26801, 31955, 31061, 28609, 31126, 32515, 31194, 30357, 28966, 30433, 28849, 31460, 31848, 26261, 31439, 29473, 29928, 29557, 26566, 29465, 30572, 27072, 31043, 28531, 30099, 31631, 26892, 27718, 27746, 27159, 27183, 28303, 29713, 29597, 26207, 30644, 26828, 28302, 30352, 31578, 27635, 32008, 26119, 26326, 26341, 31290, 30718, 27508, 29601, 29849, 27988, 27465, 29613, 29516, 31535, 30960, 30743, 26685, 31799, 28803, 32112, 27654, 31311, 27502, 26808, 28708, 26775, 31248, 31217, 30857, 28715, 28762, 28043, 28735, 28688, 32544, 29072, 28513, 27601, 29401, 27169, 28732, 30741, 32026, 26923, 30426, 28238, 27094, 30764, 32399, 32526, 27130, 31230, 28594, 28231, 32017, 28268, 29008, 29595, 29861, 29341, 32528, 27972, 28950, 28519, 26668, 31402, 27210, 31390, 30337, 32162, 27954, 27446, 26478, 31184, 28761, 31478, 29015, 30932, 27273, 30836, 31911, 26520, 32482, 30129, 28213, 28186, 29300, 28964, 26359, 31579, 26741, 29907, 29139, 32113, 28248, 27095, 27317, 29913, 30301, 28537, 27686, 28828, 31057, 30419, 27066, 26931, 28648, 27411, 30422, 28921, 26919, 28624, 31026, 26258, 26701, 27454, 32291, 29630, 30194, 30486, 30473, 30334, 30081, 28867, 27339, 31047, 26264, 31901, 30265, 30661, 27599, 31960, 30576, 32128, 31993, 27115, 29961, 29787, 30648, 26476, 28129, 28606, 32261, 28711, 26417, 29517, 27595, 31665, 29531, 28228, 29813, 28062, 31580, 26602, 32509, 28372, 26392, 26282, 28534, 30181, 30768, 26063, 29207, 32012, 26782, 27990, 27510, 28917, 31607, 27471, 27438, 28083, 30944, 29438, 31894, 30025, 29417, 28455, 28032, 29616, 29007, 27868, 31187, 27118, 29011, 26612, 30608, 27295, 32166, 30372, 27808, 32342, 27559, 28997, 30139, 31117, 32285, 32294, 27229, 31228, 28550, 30716, 26832, 27643, 31051, 28416, 26266, 27135, 28864, 28943, 30979, 26527, 28199, 27609, 31319, 26740, 31111, 29539, 26847, 29653, 28835, 29083, 31153, 30826, 28907, 27343, 29179, 28082, 31055, 27244, 31354, 29568, 27965, 31322, 26957, 27535, 31062, 31805, 31314, 32333, 27605, 32038, 27392, 26449, 26607, 26917, 28471, 32083, 31544, 26753, 32500, 29324, 29615, 28856, 30462, 30808, 29415, 31147, 26643, 31761, 27942, 26897, 31718, 29030, 28171, 31378, 28245, 30788, 28507, 32404, 29136, 29528, 26906, 31667, 30214, 30050, 28012, 30645, 26875, 26603, 26407, 31496, 28435, 26561, 27976, 31268, 31137, 29960, 30092, 28131, 31615, 26269, 32022, 31257, 29467, 30907, 28400, 29177, 32437, 29588, 32262, 28007, 32087, 29654, 31426, 27876, 30244, 30047, 31560, 29046, 28250, 27424, 27662, 27009, 30283, 31576, 32130, 27910, 29142, 26314, 27285, 30207, 29999, 30593, 27434, 30339, 26111, 30455, 29110, 30498, 28449, 30399, 31612, 30257, 31195, 27549, 26200, 28265, 26098, 28838, 31289, 28425, 27096, 26704, 27640, 29755, 28499, 30988, 31534, 31810, 30567, 29741, 29160, 30852, 28618, 27351, 27999, 28792, 26452, 30571, 31947, 30355, 31030, 29460, 26112, 29839, 29265, 29200, 31424, 29922, 28861, 28676, 32356, 28806, 27270, 30827, 26496, 32481, 28764, 29116, 29559, 27199, 31008, 31461, 27390, 29679, 31501, 29955, 28596, 32175, 26825, 31021, 32277, 31788, 32531, 32255, 28383, 27209, 30843, 29382, 32190, 30267, 26537, 29890, 29989, 26772, 27303, 27994, 29120, 28361, 27787, 28736, 29571, 28179, 30450, 27871, 26896, 28316, 27519, 30777, 29034, 32215, 26674, 32369, 29513, 27084, 28568, 32292, 29803, 29819, 27521, 32253, 28127, 31324, 29329, 32309, 26790, 30883, 32135, 31664, 30193, 28071, 27019, 30172, 32422, 30893, 31070, 32154, 32460, 30416, 29105, 30442, 31671, 29148, 31825, 29500, 27400, 26475, 27020, 32219, 29079, 27147, 31134, 32274, 30584, 32241, 30531, 28894, 31914, 31966, 26108, 28853, 27073, 26184, 29626, 30812, 27946, 32308, 27702, 30030, 29838, 27574, 31474, 29432, 26205, 31510, 30382, 26987, 32444, 30065, 30077, 27013, 28300, 26508, 29723, 26817, 27197, 30073, 30269, 29503, 31838, 30864, 32098, 27228, 31633, 32109, 31811, 30688, 32357, 27319, 29678, 29878, 27631, 29197, 28558, 30842, 27297, 27430, 28447, 27374, 31891, 31995, 26094, 27749, 26983, 30015, 31056, 30400, 30945, 30589, 26865, 27074, 28100, 30346, 32081, 32114, 28637, 27805, 27376, 29982, 30294, 30060, 32389, 31808, 30910, 28349, 32281, 30896, 30699, 27621, 31666, 27058, 31699, 30404, 30617, 28041, 32311, 29556, 28898, 31987, 26553, 32413, 29088, 28138, 27189, 30323, 30006, 31198, 29070, 29874, 29796, 31351, 27731, 29744, 26219, 27833, 32485, 28054, 29697, 27552, 26968, 30447, 26248, 27613, 29217, 31315, 28999, 29095, 29475, 27395, 31778, 26960, 26144, 29059, 29720, 31692, 28889, 27835, 27779, 26783, 26249, 30973, 29402, 31915, 26844, 31394, 30882, 26220, 31165, 31796, 27798, 32486, 30989, 31655, 28373, 28287, 30211, 26639, 30197, 28484, 29777, 27639, 31088, 26899, 30513, 27564, 28629, 29051, 26632, 27851, 29621, 29315, 29071, 26311, 28873, 31907, 32024, 27264, 26097, 27899, 27408, 32035, 32085, 31730, 31725, 29786, 32418, 30179, 30587, 29253, 27897, 31212, 32015, 31164, 28840, 31305, 31944, 28345, 30145, 28195, 27034, 27218, 30642, 31443, 31301, 27818, 30237, 30656, 29728, 26479, 26472, 27987, 29054, 26516, 28593, 30182, 32288, 29014, 26760, 26532, 27060, 26590, 27240, 30713, 29879, 26300, 30677, 29662, 30834, 29345, 26922, 30520, 31482, 32136, 29428, 31650, 26898, 30770, 30199, 30510, 29308, 27630, 30163, 27852, 26804, 31367, 28342, 30142, 30795, 30005, 28859, 27162, 27238, 30020, 29425, 27928, 27005, 29718, 26610, 27836, 26068, 31893, 28698, 28122, 26400, 29313, 29484, 29734, 32147, 32137, 26982, 26781, 26785, 32495, 30598, 27252, 29441, 28918, 31349, 30748, 28607, 28895, 27893, 30829, 27415, 31190, 31215, 27862, 28973, 27201, 29397, 29919, 26548, 27583, 28192, 30779, 30709, 29804, 31209, 27981, 26747, 30011, 29573, 32170, 30091, 26889, 26297, 29957, 31498, 28025, 31533, 26328, 27813, 28376, 31591, 27036, 27751, 26839, 29638, 29318, 29275, 28053, 26137, 27194, 32447, 26873, 26302, 27397, 27389, 29900, 28003, 28394, 29810, 28493, 26115, 31206, 32002, 29078, 29398, 29773, 30311, 26943, 31592, 26158, 30758, 28174, 30481, 29706, 29672, 29125, 28262, 32489, 31852, 28730, 27721, 28317, 32347, 30639, 30559, 31843, 28536, 32063, 26622, 28584, 30335, 27110, 29738, 29137, 31931, 26337, 28200, 26428, 32202, 29806, 28744, 29347, 28767, 28881, 26866, 31996, 27978, 32522, 27573, 32468, 27743, 29380, 28346, 32045, 30317, 29390, 31794, 30615, 26050, 30903, 26764, 30740, 30443, 29954, 27202, 30554, 28582, 26196, 27257, 32061, 31179, 31654, 26502, 30880, 29372, 28829, 30694, 28061, 28919, 30561, 29714, 30110, 31698, 32290, 31822, 26292, 26202, 26654, 28623, 31694, 26352, 29929, 29562, 27431, 31807, 28202, 30911, 29737, 26370, 29950, 29076, 31176, 29449, 28170, 28036, 30983, 26274, 29434, 30756, 30775, 30308, 26574, 29012, 29489, 30541, 26927, 30124, 29586, 31521, 31282, 31986, 27814, 30388, 26405, 31205, 26799, 30651, 31683, 27853, 31824, 31961, 30707, 28619, 28689, 31756, 32483, 32451, 28428, 28267, 30126, 29296, 27890, 32259, 26332, 26273, 31802, 30731, 29424, 28928, 26460, 29234, 26965, 28442, 26961, 31883, 31293, 30521, 30461, 29049, 26789, 29780, 27164, 28375, 32335, 28264, 31528, 26121, 29161, 30784, 30832, 31884, 27474, 28949, 30109, 27913, 29507, 30921, 27287, 30578, 29406, 28603, 30866, 31880, 30553, 30522, 29204, 30333, 29916, 28598, 31312, 27129, 28340, 31942, 29524, 29211, 31434, 30763, 27274, 29938, 29352, 31086, 31519, 26142, 31347, 31812, 32501, 28562, 29985, 28177, 30869, 27698, 30582, 29478, 28590, 26891, 32011, 30039, 32121, 30289, 30922, 32188, 27484, 28748, 26901, 32400, 32271, 27111, 32353, 30635, 31003, 27738, 27912, 28135, 30590, 28225, 26290, 28990, 31781, 32210, 27892, 29302, 29733, 30658, 27308, 29369, 28115, 26814, 28181, 29284, 32373, 26224, 27691, 31515, 31400, 31143, 31517, 28472, 27461, 26254, 28719, 31923, 26393, 30856, 28348, 31975, 29226, 32275, 26470, 28770, 30054, 27837, 26656, 30835, 30966, 26735, 30120, 29488, 31562, 26057, 31538, 32401, 28443, 26402, 31393, 31139, 32478, 30398, 29169, 31670, 30762, 30291, 30259, 29277, 31246, 31701, 30888, 32360, 26474, 31866, 28040, 27116, 27807, 27354, 26731, 29048, 32039, 32158, 31428, 31252, 27105, 29502, 30579, 31191, 31490, 29906, 32181, 31556, 30184, 29216, 28927, 26609, 29356, 28687, 27612, 29866, 28459, 29847, 27810, 27425, 31876, 31557, 28271, 28652, 28369, 28160, 29133, 27145, 31240, 28063, 26820, 26678, 32168, 26705, 27864, 32260, 29203, 29872, 27472, 27541, 27048, 32551, 32530, 26286, 29288, 29242, 26614, 27237, 29389, 30007, 29611, 31445, 31979, 27690, 29802, 27401, 31371, 28256, 27815, 30359, 32105, 28547, 27496, 30722, 31507, 32056, 29841, 29939, 30062, 32223, 28571, 30566, 27478, 27528, 28556, 29155, 29191, 29472, 31487, 31352, 28710, 31897, 30890, 31368, 27052, 31256, 28977, 28307, 31273, 27156, 26583, 27882, 31542, 26745, 30059, 26448, 28740, 30343, 31425, 30051, 32173, 31002, 27166, 29366, 29293, 28842, 30899, 26650, 27258, 30516, 26250, 31918, 26905, 28269, 28848, 27522, 32220, 28411, 28058, 28942, 26406, 30032, 30117, 26528, 28548, 30430, 26884, 30841, 28972, 26301, 26920, 26331, 26308, 31818, 32391, 32330, 27682, 31697, 28084, 31733, 26239, 27572, 30710, 31956, 32344, 31766, 28512, 29294, 32351, 28418, 27422, 31742, 27271, 28543, 32117, 31150, 29439, 31606, 30391, 30463, 30937, 26874, 29261, 29064, 26168, 31831, 29673, 29574, 28326, 30061, 28857, 26338, 28588, 30033, 26278, 31912, 30555, 31339, 26348, 27617, 30991, 27381, 26485, 32554, 27650, 31583, 32066, 31477, 27236, 29902, 28049, 28757, 26435, 30547, 26272, 31904, 29448, 30098, 30986, 27080, 29633, 29527, 32067, 29715, 31279, 29695, 31786, 29201, 28280, 28430, 31832, 27442, 29108, 29337, 27364, 28491, 29980, 28626, 30993, 29170, 28088, 31929, 32507, 32548, 30235, 31953, 26123, 32071, 27670, 29543, 29639, 26835, 31066, 27003, 31497, 28035, 29426, 29735, 28027, 29058, 29905, 28150, 29886, 31889, 30036, 27398, 31107, 26090, 31680, 30750, 31561, 31969, 32107, 28221, 28669, 27628, 30525, 27427, 29243, 31568, 31952, 29724, 29033, 28244, 30673, 27286, 32004, 26786, 29450, 31348, 26306, 26344, 30118, 27061, 27205, 26723, 29143, 32502, 27302, 32143, 29290, 30575, 26130, 29154, 27918, 27666, 31899, 26955, 29282, 31142, 27567, 30162, 31743, 27725, 26482, 32006, 27280, 31740, 29312, 27139, 27314, 26357, 29842, 26493, 28641, 26784, 30123, 28682, 29887, 28847, 31921, 27793, 32151, 31151, 26291, 28731, 31819, 30797, 28154, 27710, 27717, 31085, 27668, 31618, 28440, 30751, 32328, 30974, 31263, 30173, 32238, 27137, 28463, 28098, 29754, 30226, 26618, 31863, 31494, 29681, 26867, 31539, 29764, 27739, 31456, 26462, 29865, 32189, 27383, 31437, 27782, 27969, 27652, 30514, 29447, 27915, 29789, 29047, 30251, 32443, 29628, 26221, 27576, 27919, 27299, 29978, 32366, 29533, 29844, 26372, 28615, 28585, 27760, 31173, 30674, 29280, 26062, 28068, 26080, 27953, 30064, 31435, 30389, 27068, 26744, 30056, 27565, 26717, 27716, 31630, 27335, 26510, 28085, 30628, 26092, 26469, 28052, 29445, 30168, 27046, 29335, 29174, 30287, 28549, 31693, 26640, 31973, 27165, 30817, 30807, 31121, 31716, 31141, 26371, 26439, 26638, 31989, 30371, 30691, 28450, 32511, 31118, 30726, 29086, 27800, 27966, 29391, 26525, 31815, 32549, 26981, 26461, 27313, 31022, 31053, 31326, 27462, 26507, 31705, 28397, 30948, 29077, 30528, 27823, 31635, 29404, 26109, 30678, 28578, 30364, 28167, 32197, 30904, 31442, 31859, 28951, 31101, 32159, 28866, 32284, 26106, 29093, 28230, 30702, 26657, 29963, 26504, 29880, 26718, 30384, 30853, 32491, 29987, 28242, 31327, 27184, 31399, 32184, 31423, 29947, 28620, 29476, 32048, 29195, 31427, 31644, 27369, 26560, 32209, 30924, 29800, 30969, 26465, 31222, 26132, 31783, 26069, 28296, 29732, 32408, 26971, 29474, 31657, 29684, 26794, 30577, 29210, 26963, 29585, 27337, 26605, 29805, 28994, 30861, 31201, 28737, 32211, 26587, 28518, 29393, 28103, 28756, 30292, 32044, 31294, 30016, 26823, 32421, 29612, 28647, 26323, 29600, 29949, 27012, 27688, 32337, 31436, 30539, 31587, 29299, 32402, 26140, 32057, 27872, 29115, 27995, 29003, 26433, 27527, 29111, 31381, 26941, 29090, 28822, 26978, 30946, 26663, 29479, 29463, 28258, 29663, 28343, 28485, 26114, 31221, 30067, 30103, 30666, 27558, 27752, 28787, 27657, 27726, 26498, 26677, 30804, 27325, 26360, 27585, 28839, 30828, 26738, 28408, 32279, 29545, 27443, 29094, 31260, 29876, 28540, 28215, 28118, 28204, 26228, 29742, 30700, 29669, 32153, 32150, 26138, 27513, 31060, 27977, 31922, 31871, 29172, 28721, 31721, 29400, 26582, 28538, 31300, 31595, 32163, 27700, 28023, 27783, 29013, 31851, 28210, 27044, 29575, 27091, 27340, 29771, 31641, 31850, 28092, 27767, 30951, 31735, 30472, 30390, 26526, 26556, 28551, 29750, 27934, 29101, 29749, 27412, 31864, 26924, 28564, 32092, 26637, 30733, 29682, 28741, 30234, 27518, 26564, 26750, 27371, 31552, 29937, 31739, 32263, 26156, 30925, 28535, 26544, 32436, 29297, 30133, 29099, 29138, 30019, 28679, 32438, 28602, 28378, 26936, 30934, 30540, 31389, 26127, 30151, 30806, 28201, 31906, 31582, 30048, 27546, 27997, 31765, 26550, 30075, 30918, 30800, 31382, 31640, 29707, 29687, 26970, 31625, 28479, 28492, 32461, 26186, 27600, 30421, 27082, 26082, 27579, 27268, 26952, 31049, 29812, 27344, 28111, 27889, 30790, 29641, 26087, 29436, 28989, 28693, 30914, 29112, 28891, 28398, 30877, 31232, 29607, 29946, 31392, 30324, 26627, 29026, 28675, 27357, 31441, 32305, 29519, 28365, 30786, 27658, 31296, 27849, 31316, 30410, 32072, 30725, 26095, 30035, 31120, 30565, 31888, 29619, 31648, 28134, 28333, 31045, 31727, 30837, 26148, 29610, 31375, 27829, 27932, 28016, 31265, 28015, 29643, 29492, 28611, 28714, 29028, 29834, 30378, 31166, 28124, 30831, 31691, 27594, 26048, 27763, 31340, 29388, 30594, 27873, 29165, 31526, 30155, 31470, 26198, 29858, 27704, 26327, 32222, 27283, 26212, 32506, 29309, 27338, 29991, 29897, 26116, 27222, 26410, 29043, 29598, 28555, 29736, 31623, 26436, 32439, 26710, 27869, 28924, 31744, 29311, 30127, 26534, 26665, 32363, 28723, 32247, 32191, 32036, 26071, 31125, 27292, 32493, 27452, 28145, 30854, 26412, 26758, 29163, 27984, 28877, 26376, 30043, 29870, 31840, 26208, 27370, 28309, 31879, 26487, 30581, 26935, 31619, 29084, 26984, 28331, 29617, 26506, 27051, 26445, 30659, 30161, 28899, 30958, 28069, 26722, 30239, 31566, 29649, 29791, 31103, 30271, 32236, 31763, 29646, 27627, 30295, 30619, 26380, 28586, 32385, 27665, 28222, 32470, 32540, 27629, 28576, 31574, 28871, 27867, 28969, 28121, 32160, 29579, 27081, 29220, 31391, 28696, 32426, 31722, 27023, 27128, 26270, 28013, 27277, 30153, 26104, 26822, 32068, 26810, 32484, 30574, 29514, 31917, 28278, 28700, 28565, 27530, 31532, 29010, 32097, 32378, 30424, 28678, 30057, 29016, 31199, 28651, 30068, 29856, 30228, 27047, 32341, 28093, 32547, 27737, 28448, 31708, 28429, 26551, 32386, 30044, 30134, 31874, 29701, 27950, 30138, 29001, 30219, 29102, 30685, 27834, 32412, 26347, 27109, 27634, 30606, 28814, 32276, 31951, 32533, 32534, 27492, 27680, 31785, 27888, 32246, 28401, 30631, 28908, 26974, 32283, 27097, 28592, 26268, 28938, 31992, 28957, 31988, 29430, 29069, 27226, 26416, 27457, 30243, 31175, 28483, 30089, 26592, 26716, 30201, 30717, 30875, 26615, 31773, 27107, 27375, 31925, 29689, 28306, 30227, 30839, 29831, 32475, 29629, 30794, 29511, 30002, 28639, 26956, 30490, 26409, 29131, 30964, 31014, 32089, 26995, 28482, 28037, 27723, 30957, 31976, 32179, 31410, 32028, 30338, 28462, 29745, 29224, 31690, 27293, 26231, 32100, 30809, 28180, 28981, 29440, 32504, 26453, 31745, 26260, 27124, 30004, 29029, 30604, 26363, 30978, 26593, 30536, 31291, 27331, 29941, 31685, 28638, 30190, 28236, 27707, 28117, 30761, 29921, 32020, 30487, 26770, 26543, 30927, 29354, 31204, 30448, 31210, 29525, 26533, 27733, 26554, 29835, 30711, 26870, 31113, 28188, 27993, 31550, 27219, 29446, 27825, 30687, 26690, 27288, 26211, 32550, 27031, 31035, 29583, 29444, 30017, 27149, 27359, 31160, 26621, 26293, 30847, 31994, 28079, 27900, 26259, 31748, 27756, 32206, 26356, 26916, 31309, 31255, 32014, 27801, 27195, 31264, 29722, 31298, 32498, 27086, 27087, 28668, 29196, 29793, 29645, 26283, 27011, 32416, 28934, 29285, 32316, 31224, 28393, 27858, 29250, 28299, 30585, 30810, 26483, 31762, 32010, 31784, 26464, 29685, 28046, 26051, 28979, 28319, 27720, 26673, 27409, 29542, 27200, 28801, 32064, 27880, 27645, 30811, 27382, 28956, 31800, 29530, 27085, 27967, 31247, 30406, 31457, 30171, 29247, 26089, 28311, 28649, 30962, 30022, 28794, 32345, 27479, 26175, 31018, 30735, 27042, 26085, 30947, 32231, 31853, 28288, 29783, 31653, 26625, 28001, 27602, 27891, 27905, 28301, 27857, 30368, 30304, 26519, 29045, 32208, 30141, 31485, 29348, 29214, 31186, 26503, 29123, 28422, 30018, 28883, 31905, 31892, 32005, 27845, 28243, 31459, 28930, 26818, 26312, 31370, 26944, 31525, 29305, 30782, 28028, 29566, 26886, 31236, 32192, 31468, 26911, 26780, 30776, 29797, 27113, 27547, 31418, 31586, 27378, 30757, 31809, 29661, 30457, 28234, 28120, 26190, 31110, 29657, 29506, 27043, 28511, 30965, 27406, 31122, 27301, 30599, 29443, 28915, 28936, 29190, 28149, 29666, 29622, 29480, 31676, 30105, 30080, 30507, 31168, 28341, 27350, 29554, 29130, 28884, 31429, 31775, 32350, 27788, 29209, 30916, 26915, 26966, 31417, 30344, 28305, 28858, 27620, 26840, 28681, 26267, 26584, 31277, 28673, 31571, 31605, 31489, 28067, 29055, 27828, 29536, 28097, 26940, 30942, 26878, 28353, 28209, 29845, 30111, 32001, 31072, 29411, 32327, 30328, 27898, 30879, 26180, 27504, 29021, 29540, 30542, 26492, 26153, 27935, 31647, 29512, 30475, 26930, 28164, 26126, 30147, 30170, 29945, 26950, 28470, 30714, 30950, 27429, 26255, 32131, 30684, 30719, 27955, 28426, 29577, 29759, 30876, 30256, 29523, 26501, 27729, 32542, 28686, 26809, 31469, 30177, 30697, 26227, 29644, 27405, 27768, 31149, 26648, 31632, 29688, 28454, 29851, 26895, 30769, 28282, 26444, 32394, 28263, 29225, 30375, 27947, 28325, 26852, 30143, 28627, 28778, 27064, 28597, 27171, 30972, 27460, 27008, 30952, 27732, 26195, 26055, 31972, 29625, 26242, 30496, 27775, 26756, 32310, 28360, 28827, 28460, 29431, 31297, 30114, 32141, 29414, 27388, 29117, 30503, 30409, 26361, 27117, 26065, 30954, 27245, 28845, 26122, 31751, 32359, 26774, 30670, 28096, 28275, 26763, 31541, 28574, 30509, 30137, 28724, 31172, 28141, 26541, 32349, 28992, 31967, 26072, 27940, 30412, 27819, 28223, 31369, 32058, 26471, 28621, 27511, 26187, 26489, 29698, 26900, 32535, 30436, 31589, 28729, 29716, 31185, 31597, 31981, 27279, 31414, 31646, 28722, 28510, 28614, 29198, 26531, 31278, 28987, 31770, 29081, 30403, 31366, 29811, 27516, 32326, 31741, 31094, 31854, 29933, 32267, 27017, 26386, 29307, 31737, 28753, 31304, 29180, 30037, 28370, 27039, 28286, 31006, 30840, 26719, 26252, 29971, 31067, 30675, 29694, 27827, 31211, 27448, 29731, 30470, 27336, 32249, 26879, 27393, 27134, 30402, 30917, 31054, 30238, 28905, 32217, 28486, 30862, 30232, 30086, 32280, 27881, 31723, 26787, 30477, 32230, 26815, 27520, 27223, 29817, 27275, 27550, 32226, 28381, 31156, 31091, 27309, 31797, 30149, 28189, 31196, 32099, 31092, 27859, 31546, 29320, 28194, 31413, 29635, 26235, 27089, 29359, 29555, 27453, 29792, 30275, 28147, 32289, 30850, 29140, 31527, 29152, 26795, 30783, 29074, 26430, 32079, 27765, 32410, 27653, 29815, 27247, 30305, 30505, 26346, 32312, 31878, 26752, 27766, 26163, 31283, 29358, 26939, 28788, 31849, 31997, 30549, 27744, 30374, 29482, 26969, 27908, 28161, 26193, 31097, 27884, 29648, 31782, 27974, 31575, 29338, 31463, 26418, 27363, 31356, 30765, 26243, 28947, 31338, 31306, 28355, 32216, 26659, 26837, 28633, 28344, 32078, 28148, 27777, 29830, 29636, 29667, 29668, 29470, 30415, 26288, 27468, 26687, 29951, 30069, 27794, 29096, 28451, 26693, 31299, 31696, 29368, 29107, 26265, 31573, 30102, 28791, 31593, 31982, 27786, 27491, 26233, 28777, 28521, 26093, 27187, 27598, 31386, 27148, 26155, 31318, 26426, 26299, 27466, 30753, 26135, 29175, 31826, 27608, 29776, 30353, 32472, 27157, 28030, 26382, 28573, 28569, 29379, 26451, 27163, 29129, 28846, 26374, 32032, 27962, 28481, 27804, 27153, 30174, 28412, 31285, 27101, 28657, 31226, 30676, 30712, 32018, 31355, 27141, 29914, 28817, 26177, 26176, 28655, 28545, 30362, 31373, 27321, 32419, 28617, 30070, 29871, 26765, 26446, 27525, 31218, 28489, 32198, 31384, 27667, 29690, 28136, 30363, 30100, 32355, 31473, 28528, 28991, 27762, 27188, 26403, 32225, 30851, 27300, 27923, 27152, 27593, 30008, 31803, 27464, 27144, 26209, 27728, 32229, 28008, 28190, 31569, 27676, 29098, 26408, 29958, 31719, 31239, 32371, 29700, 30629, 31760, 31357, 29000, 32235, 30058, 28026, 28359, 31102, 29814, 32538, 26709, 27065, 27850, 31801, 28718, 31938, 28685, 28332, 28074, 31610, 32062, 27224, 30316, 26565, 29952, 31039, 31926, 29807, 31736, 30824, 30640, 32133, 30435, 27004, 30090, 26240, 28270, 28833, 29620, 28086, 27410, 27233, 26179, 29538, 30928, 28391, 27789, 32430, 26305, 27709, 30076, 26294, 30432, 28946, 31157, 26309, 27956, 27006, 30627, 26850, 31757, 30246, 31677, 30189, 28790, 28666, 31688, 26568, 30570, 28843, 26767, 28439, 30459, 30379, 27455, 26429, 31106, 28625, 31202, 27204, 26217, 32441, 28072, 31594, 29581, 31350, 27904, 26367, 29469, 27192, 30437, 27470, 28374, 29892, 29883, 31069, 30113, 31182, 26120, 27333, 31362, 31476, 30600, 31684, 27590, 29371, 31636, 30314, 27855, 32415, 28227, 26559, 32358, 27615, 27927, 26192, 28646, 29205, 32142, 27215, 26856, 31027, 28870, 28613, 30759, 27722, 27841, 31365, 31453, 31359, 32409, 27311, 29023, 30532, 26986, 27902, 32119, 30423, 29918, 28643, 30074, 28237, 29984, 29114, 30055, 27938, 30923, 28970, 27413, 30845, 30225, 32494, 26739, 28520, 27069, 29664, 29164, 26796, 31000, 29330, 27399, 29422, 26355, 26349, 27078, 28347, 26287, 28277, 29134, 27831, 32453, 29895, 26578, 28750, 28759, 29263, 28546, 27545, 30556, 26322, 27186, 32397, 31128, 28911, 31820, 29248, 26281, 32296, 27015, 26124, 27212, 32122, 28336, 31374, 30306, 28339, 32242, 28119, 29650, 26497, 28191, 32204, 31937, 27937, 30728, 29943, 28798, 32102, 27276, 31964, 27132, 27769, 28304, 28922, 31910, 26826, 32474, 29189, 28805, 26067, 26563, 27754, 31041, 27625, 29408, 28322, 31506, 32325, 31407, 26099, 30107, 27712, 29461, 29268, 30156, 26773, 30497, 27618, 32054, 29258, 27018, 28314, 27655, 30454, 27180, 27123, 28321, 27396, 30891, 31643, 31171, 28642, 29146, 31259, 31440, 28544, 26669, 28746, 30063, 27254, 28423, 26938, 31238, 30413, 29193, 26626, 28207, 30220, 30803, 31793, 32320, 29423, 28166, 30034, 31411, 30377, 26182, 26653, 31759, 28591, 32518, 27542, 32329, 26350, 29769, 28452, 26951, 26477, 30529, 32041, 31946, 26369, 31343, 27753, 27090, 28249, 31130, 31281, 28836, 29591, 26091, 26319, 30187, 29149, 28387, 30223, 30963, 32174, 30535, 26909, 27681, 27727, 29829, 26651, 30610, 29278, 29593, 27368, 30638, 30446, 26619, 32207, 30021, 30192, 29365, 32367, 32317, 31019, 27483, 31958, 29781, 32252, 30867, 32442, 26681, 31943, 27958, 26846, 31404, 29462, 31448, 32196, 30591, 30621, 32403, 29677, 31406, 32560, 29827, 28691, 28357, 31446, 32559, 32512, 27253, 32140, 28029, 28042, 28622, 26597, 30970, 26696, 26601, 28255, 26113, 30825, 30071, 30515, 29808, 26634, 30537, 27536, 26191, 27839, 31237, 27436, 26411, 32155, 26383, 29988, 26165, 28113, 26289, 30146, 31660, 30597, 30526, 26511, 27761, 29244, 29569, 29969, 26423, 30920, 31188, 31076, 27175, 28820, 27524, 30650, 27170, 26210, 28358, 27373, 26178, 31512, 27307, 28760, 26616, 28751, 29618, 30860, 28364, 26599, 28929, 27610, 28739, 29182, 30884, 30820, 29656, 26913, 27734, 31971, 27380, 27695, 30690, 31673, 27673, 29775, 30609, 29550, 28076, 26918, 28351, 26996, 29867, 31310, 30354, 30544, 31830, 27239, 26215, 29481, 30282, 26316, 31873, 30909, 27007, 28431, 30469, 30115, 26754, 31044, 31396, 30444, 27570, 27242, 28313, 31358, 26375, 30332, 26703, 28197, 27877, 26979, 31516, 32307, 26078, 29236, 28683, 31524, 27262, 30580, 27428, 26749, 26596, 31668, 30729, 28094, 30801, 26912, 28281, 26725, 30772, 32172, 29761, 30116, 29683, 30792, 26985, 28021, 28018, 29521, 32243, 27290, 31104, 27533, 27298, 27503, 26340, 27883, 29696, 26324, 30734, 28495, 28380, 29158, 30083, 26522, 30268, 27119, 29565, 27214, 28716, 30302, 27708, 28985, 26131, 26329, 28799, 26073, 29894, 30885, 28075, 26768, 32268, 27444, 31714, 26413, 31131, 26172, 26313, 31608, 31372, 29801, 30919, 31687, 31138, 28890, 26771, 27757, 30326, 31662, 27198, 26588, 26853, 30166, 28863, 29252, 27532, 29041, 31063, 31023, 29974, 32393, 26318, 28203, 27083, 32529, 26454, 32034, 27306, 28324, 31119, 27531, 31177, 26683, 29632, 28774, 26990, 26333, 29748, 31827, 29518, 31675, 28677, 30495, 32398, 31028, 30285, 27294, 28196, 26529, 27121, 26904, 28834, 29367, 26481, 26542, 27126, 26223, 31529, 31203, 27584, 27246, 30131, 29292, 31084, 26836, 32094, 29751, 30439, 28217, 29788, 31816, 26056, 26334, 31258, 30130, 26491, 28407, 30242, 28504, 29676, 27998, 26736, 30720, 29857, 30560, 27334, 27146, 30912, 32301, 31882, 28903, 32069, 27856, 28684, 26315, 29238, 26084, 30322, 27741, 31933, 26358, 29998, 26151, 28162, 29159, 32194, 27683, 28173, 28962, 29332, 28059, 31814, 27692, 29603, 30622, 26869, 28879, 28064, 32059, 31567, 31909, 31059, 26954, 28595, 29349, 26256, 27220, 27982, 29825, 30188, 27485, 30079, 26864, 28660, 26934, 31823, 32496, 31262, 27748, 29822, 30478, 26189, 30327, 31116, 29019, 31405, 29997, 29634, 28670, 26860, 26307, 28279, 28476, 28604, 31558, 30176, 26937, 29970, 27529, 26431, 31363, 27979, 32178, 27963, 32213, 30643, 27971, 26421, 29229, 27037, 30320, 31320, 26161, 32124, 32183, 29459, 30157, 27677, 29166, 26686, 28971, 28958, 26377, 27941, 32467, 29325, 31787, 30296, 30603, 29882, 28329, 27735, 32384, 29373, 31174, 27571, 26317, 31046, 28031, 28906, 26821, 28114, 30849, 30213, 32322, 31360, 27553, 29995, 31017, 31747, 29222, 30939, 28176, 30996, 31280, 32221, 26509, 31337, 29162, 27489, 29596, 28112, 30210, 32212, 31004, 26521, 29923, 29150, 28982, 27556, 28185, 27506, 27603, 27870, 32411, 27185, 30148, 26419, 28517, 31154, 32433, 30361, 29458, 26953, 27206, 27150, 29230, 31669, 28976, 29660, 29967, 26581, 28996, 31421, 29410, 32315, 27806, 29765, 31731, 32177, 31620, 28078, 31328, 26388, 29821, 28474, 27523, 31611, 31159, 26102, 27750, 27844, 27648, 31471, 31777, 28926, 30543, 28501, 30679, 27420, 26238, 30300, 28975, 30212, 30261, 31444, 29768, 29066, 26613, 32232, 32182, 31749, 29035, 29322, 28272, 32516, 32462, 28703, 31377, 30121, 28800, 31695, 26733, 29340, 30892, 29396, 27517, 30441, 29103, 27544, 29799, 32101, 27312, 26222, 26171, 28978, 31266, 28749, 29547, 26384, 32256, 28417, 29037, 28292, 28914, 30042, 27817, 27866, 27330, 32095, 29990, 28605, 26843, 26635, 26660, 26617, 31887, 28125, 27323, 29823, 30277, 27120, 26549, 28720, 26500, 28984, 31274, 32457, 29977, 30200, 30205, 26378, 28367, 27131, 31484, 31514, 31645, 31491, 28456, 26166, 30501, 27557, 29564, 26838, 26848, 26949, 28701, 31674, 26658, 31034, 30692, 31096, 26066, 27699, 32282, 32361, 28415, 27696, 31244, 28312, 30773, 32080, 32269, 31090, 28707, 31928, 31454, 30031, 32519, 26811, 28155, 30229, 28497, 30868, 28133, 29310, 27582, 28480, 28261, 27133, 30557, 28266, 26833, 28140, 30263, 26769, 29863, 28327, 27906, 30752, 29144, 28320, 28045, 27154, 26641, 30683, 31178, 28168, 28247, 32379, 26571, 31998, 31520, 29183, 27476, 29466, 29898, 32459, 28533, 31108, 32025, 29942, 28875, 26859, 30482, 28453, 27421, 29966, 32321, 28940, 29757, 29068, 28436, 27433, 30908, 32396, 28184, 30696, 30846, 29004, 31999, 28713, 29691, 27507, 30641, 26751, 29592, 27379, 32546, 30027, 28645, 31656, 30887, 29891, 28572, 32125, 27456, 30749, 30657, 27498, 28600, 31715, 26234, 26145, 29212, 28783, 26862, 28831, 30913, 29231, 32428, 28295, 28810, 26885, 27715, 28289, 32050, 26999, 29674, 28852, 31397, 27799, 31123, 30855, 26729, 30873, 30299, 27555, 31227, 29816, 32469, 26872, 29665, 28395, 26868, 27207, 30760, 31600, 31702, 28402, 28107, 31412, 32016, 30704, 27637, 27451, 27021, 26730, 28808, 27234, 31005, 26539, 26688, 28390, 30562, 28923, 28224, 26604, 31095, 31105, 26628, 27320, 26128, 28218, 31789, 26580, 29457, 31920, 30747, 28101, 31977, 31678, 28851, 26988, 31577, 26401, 26387, 31504, 32003, 32214, 31717, 26827, 26310, 29760, 28824, 31447, 31908, 30858, 27740, 27318, 26275, 30358, 26075, 26517, 28844, 27684, 30386, 26188, 31071, 27770, 30745, 28671, 28775, 30816, 32029, 28384, 30848, 30805, 27256, 26694, 32199, 27160, 30499, 31738, 28458, 27714, 29273, 27903, 29589, 29680, 31032, 32176, 28496, 30981, 27548, 27127, 31686, 32318, 31334, 27497, 28692, 32046, 31500, 30221, 27332, 28051, 30997, 28274, 32304, 31492, 29973, 28974, 32306, 29655, 32372, 31509, 32070, 28910, 30680, 26415, 27745, 30538, 27203, 27259, 27649, 28354, 30793, 31713, 27057, 32541, 28587, 28257, 31679, 30815, 28379, 27386, 29385, 32278, 29355, 31080, 29740, 29772, 28705, 30984, 28854, 28772, 28090, 27284, 30231, 26457, 32042, 28475, 31769, 27352, 31335, 29920, 26893, 29106, 31875, 30178, 28552, 27785, 29188, 29889, 31772, 29412, 31682, 28872, 27033, 28522, 30682, 32473, 32435, 26214, 27758, 30870, 26365, 32429, 27543, 31885, 26160, 32052, 28656, 30160, 31927, 26792, 26110, 30493, 26368, 31555, 31865, 29996, 32023, 30376, 29279, 29520, 29496, 30823, 28963, 29221, 32334, 31042, 30297, 28780, 29344, 28363, 31148, 30844, 26734, 30592, 26197, 29881, 27327, 28445, 31845, 30994, 29535, 27663, 30293, 31129, 32336, 27255, 30724, 30796, 32195, 28813, 31553, 26514, 29608, 27414, 27626, 28356, 29456, 26246, 32118, 32108, 26480, 26707, 27968, 26998, 28717, 28754, 28050, 27341, 27391, 32525, 29421, 26980, 26442, 31537, 26253, 29992, 31438]\n",
            "age                                NaN\n",
            "fnlwgt                             NaN\n",
            "education_num                      NaN\n",
            "capital_gain                       NaN\n",
            "capital_loss                       NaN\n",
            "                                  ... \n",
            "native_country_ Thailand           0.0\n",
            "native_country_ Trinadad&Tobago    0.0\n",
            "native_country_ United-States      1.0\n",
            "native_country_ Vietnam            0.0\n",
            "native_country_ Yugoslavia         0.0\n",
            "Name: 28956, Length: 100, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55RxSR4sCql8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "6e3128ba-7e86-4f69-96bf-37e17a5eeb4d"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=2)\n",
        "kmeans.fit(X_train)\n",
        "y_kmeans = kmeans.predict(X_train)\n",
        "\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n",
        "\n",
        "plt.show()\n",
        "\n",
        "### END YOUR CODE"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-43f5512926ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_kmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"C\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n\u001b[0;32m--> 859\u001b[0;31m                         order=order, copy=self.copy_x)\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;31m# verify that the number of samples given is larger than k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cYh51FqCql-",
        "colab_type": "text"
      },
      "source": [
        "#### YOUR VERBAL SOLUTION HERE\n",
        "\n",
        "\n",
        "\n",
        "#### END YOUR VERBAL SOLUTION HERE"
      ]
    }
  ]
}